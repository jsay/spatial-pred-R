#+TITLE:       Extending Predictions from Spatial Econometric Models on R
#+AUTHOR:      Jean-Sauveur AY \\ \lt[[mailto:jsay.site@gmail.com][jsay.site@gmail.com]]\gt \and Raja CHAKIR\\ \lt[[mailto:chakir@grignon.inra.fr][chakir@grignon.inra.fr]]\gt\\ \and Julie LE GALLO\\ \lt[[mailto:jlegallo@univ-fcomte.fr][jlegallo@univ-fcomte.fr]]\gt 
#+LaTeX_CLASS: ManueStat
#+OPTIONS:     tags:nil toc:nil H:5
#+STARTUP:     hideblocks
#+BIND:        org-latex-image-default-width ""
#+BIND:        org-latex-tables-booktabs t
#+PROPERTY:    session *R*
#+PROPERTY:    exports both
#+PROPERTY:    eval no
#+INFOJS_OPT:  view:showall toc:nil ltoc:nil mouse:underline path:http://thomasf.github.io/solarized-css/org-info.min.js
#+HTML_HEAD:   <link rel="stylesheet" type="text/css" href="http://thomasf.github.io/solarized-css/solarized-light.min.css" />
# HTML_HEAD:   <base target="_blank">
#+BEGIN_abstract
This document presents an integrative framework to make predictions
from spatial autoregressive models \cite{COrd73, COrd81, Anse88}. It
also contains the corresponding [[url:http://www.r-project.org][*R* code]] to implement the predictors
presented. The code is tangled into the [[./sppred.R][=sppred=]] function that
implements in particular the predictors from LeSage and Pace
(2004, 2008) and Kelejian and Prucha (2004) for a large number of
specifications of spatial autocorrelation from the [[url:http://cran.r-project.org/web/packages/spdep/index.html][=spdep= package]]
(Bivand 2014). To use this code, save the file [[./sppred.R][=sppred=]] in your
working directory then submit =source("sppred.R")= to *R*. Some
examples on the =boston= data are also available at section XX. The
status of this work is actually under construction, comments are
welcome.\\

*=STILL TODO=* 
- Code the variances and confidence intervals of predictors
- Code the weight matrix for ex-sample predictions
- Code the predictors for =sphet= and =splm= objects
#+END_abstract
#+TOC: headlines 1

  # @Article{RePEc:eee:jhouse:v:21:y:2012:i:1:p:66-76,
  # author={Kato, Takafumi},
  # title={{Prediction in the lognormal regression model with spatial error dependence}},
  # journal={Journal of Housing Economics},
  # year=2012,
  # volume={21},
  # number={1},
  # pages={66-76}

* Theoretical Framework
** Spatial econometric models

   The particularity of spatial econometric models is the /ex ante/
   specification of interdependence between statistical units,
   typically from their spatial proximity.  Consequently, resulting
   predictions not only depend on the variables of the target units
   (i.e., the units for which we want to predict the outcome) but
   potentially involve the entire set of both explanatory as well as
   dependent variable sample data observations.  This implies that the
   sample pattern of the outcomes or the residuals contains additional
   information which may be used to modify the regression function so
   as to reduce the prediction variance (Goldberger 1962). [[cite:COrd73]]

   The literature about predicting from spatial econometric models is
   not actually unified, due to different modeling frameworks and
   notations: see LeSage and Pace (2004, 2008) and Kelejian and Prucha
   (2004).  A first contribution of this document is to unify the
   different predictors proposed in the literature into an integrative
   spatial econometric framework.  A second contribution is to present
   the *R* function =sppred= that implements all the spatial
   predictors and presents some empirical evidence on their relative
   performance on a well-known dataset (=boston=, Bivand 2014).

   We present analytically the available predictors from spatial
   econometric models estimated on a sample of $N$ spatial units. We
   begin with the more general spatial autoregressive mixed
   conditional (SMC) specification of the Cliff-Ord (1973, 1981) class
   of homoscedastic models with exogenous covariates,[fn:1]

\begin{align}
y & = \alpha+ \rho Wy+X\beta+ WX\theta+ u\nonumber\\
u & = \lambda W u+ \varepsilon \;\;\mbox{ with }\;\; \varepsilon\sim \mathbf{N}(0, \sigma^2I_N)\nonumber
\end{align}

   The term $y$ is a $N\times 1$ vector of the continuous outcome of
   interest, $X$ is a $N\times K$ matrix of the $K$ non-constant
   covariates, and $W$ is a $N\times N$ full-rank spatial weight
   matrix (see Anselin 1988). We limit ourselves to a same weight
   matrix in the outcome and error equations, but formally nothing
   requires this restriction. The unknown parameters (or vectors of
   parameters) $\alpha$, $\rho$, $\beta$, $\theta$, $\lambda$ and
   $\sigma$ grouped in $\Theta$ have to be estimated, as the vector
   $\varepsilon$ of innovations. Classically, we assume that
   $\mbox{diag}(W)= 0$ and $|\,\rho\,|$, $|\,\lambda\,|< 1$. The
   spatial weight matrix $W$ does not need to be symmetric.

   This form is sufficiently general that the spatial autoregressive
   conditional (SAC) model can be recovered with $\theta= 0$, the
   spatial Durbin model (SDM) model with $\lambda=0$, the spatial
   autoregressive (SAR) model with $\theta=\lambda=0$, the spatial X
   model (SXM, also called Spatial Durban Error Model by LeSage and
   Pace, 2009) with $\rho=0$ and the spatial error model (SEM) can be
   recovered with $\rho=\theta= 0$. All these models can be estimated
   by maximum likelihood through the functions =errorsarlm=,
   =lagsarlm=, and =sacsarlm= of the *R* package =spdep= (Bivand,
   2014).

   For future reference, note that the general SMC model can be
   reduced in $y= (I-\rho W)^{-1}X\beta +(I-\rho W)^{-1}(I-\lambda
   W)^{-1}\varepsilon$, with a distribution that involves all of the
   model parameters:

\begin{equation}
y\mid X, W, \Theta \sim \mathbf{N}(\mu, \sigma_{\varepsilon}^2\Sigma),
\end{equation}

   with $\mu=(I-\rho W)^{-1}X\beta$, and $\Sigma= \big[(I-\rho
   W)(I-\lambda W)(I-\lambda W^\top)(I-\rho W^\top)\big]^{-1}$.  With
   this writing, we can distinguish the non-stochastic and the
   stochastic components of the observations, which are of central
   interest in unifying the different predictors.  According to the
   literature, we consider hereafter that the set of parameters
   $\Theta$ is known with certainty.  This has the advantage of
   simplifying notations by neglecting a source of uncertainty that
   depends on the sample size.  Moreover, all the parameters are
   estimated simultaneously and it makes no sens to consider $\Theta$
   as partially known.  Hence, the predictors presented are all
   conditioned by $\Theta$, which is dropped from conditioning sets
   for the sake of clarity. So its not the true BLUP but an
   approximation.

   Finally, note that geo-statistical models (typically from the
   kriging methods, \cite{CCas93}) are aware about the need for using
   BLUP predictors.  Relatively to spatial econometric approaches,
   they involve specifying spatial dependence through the error
   process (like in spatial error models, see below) as opposed to the
   spatial lag of the outcome vector.  Predicting from these error
   models take a simpler form than autoregressive models and can be
   easily recovered from the theoretical framework presented here. The
   reverse is not true.[fn:3]

** Implementing predictions

   Basically, we consider $M$ target spatial units for which we want
   to predict the values of the outcome $y$.  These units may or may
   not be included in the dataset used for the estimation of the
   parameters (that we call focal units).  At this moment, it makes no
   differences and we distinguish them only in the research interest
   of obtaining predictions of $y$.[fn:2] We discuss this point in
   more details at the next subsection XX.

   The available information set is crucial here, as each predictor is
   formulated as a conditional expectation relative to it. 

. in terms of conditional expectation put the focus of
   the available information used to construct the predictors which is
   important for the transparency of the code. 

\begin{equation}
\tag{PRD.X}\mathbf{E}(y\mid X)= X \widehat{\beta}\nonumber
\end{equation}

\begin{equation}
\tag{PRD.WX}\mathbf{E}(y\mid X, W)= X\widehat{\beta}+ WX\widehat{\theta}
\end{equation}

\begin{equation}
\tag{PRD.KP1}\mathbf{E}(y\mid X, W)= (I_n-\rho W)^{-1} (X\widehat{\beta}+ WX\widehat{\theta})
\end{equation}

   This correspond to KP1 but also the exogenous predictor of LSP. It
   is the default predictor of the function =sppred=.

\begin{equation}
\tag{PRD4}\mathbf{E}(y\mid X, W, Wy)= \rho Wy+ X\widehat{\beta}+ WX\widehat{\theta}+ \lambda (Wy- X\beta- WX\theta)
\end{equation}

   It is a bit strange but $y$ cannot be recovered from $W$ and
   $Wy$. KP analyses this predictor separately in KP4 and KP5 but such
   a distinction is not necessary.  Since $\mbox{diag}(W)=0$, $W y$
   does not use individually the $y_i$ to predict itself but needs
   them all to predict the entire vector.

   The bias of actual predictors come from the correlation between the
   spatially lagged dependent variable and the error term. It is why
   the use of best linear unbiased predictor (Golberger) is of
   particular importance.

\begin{equation}
\tag{PRD5}\mathbf{E}(y\mid X, W, y_{IS})= (I_n-\rho W)^{-1} X\widehat{\beta}+ WX\widehat{\theta}
                                        + \Omega_{21}\Omega_{11}^{-1}(y_{IS}-\mathbf{E}(y_{IS}\mid X, W))
\end{equation}

   The covariance can be simplified with precision matrix (Harville)
   which do not necessitate the inversion of matrix:
   $\Omega_{21}\Omega_{11}^{-1}= \Psi_{21}\Psi_{11}^{-1}$. LSP also
   present a leave-one-out (loo) specification of this predictor, that
   is based on the same conditioning set, except that we consider the
   predictions individually of each OS units indexed $i$.

\begin{equation}
\tag{prdLSP}\mathbf{E}(y_i\mid X, W, y_{IS})= (I_n-\rho W)^{-1}X\widehat{\beta}+ WX\widehat{\theta}
                                            + \Omega_{21}\Omega_{11}^{-1}(y_{IS}-\mathbf{E}(y_{IS}\mid X, W))\label{prdLSP}
\end{equation}

   This simplification comes from...

   Note that this predictor is not consistent in terms of expectation
   conditioned by a unique information set. In effect, the
   non-stochastic part is conditioned without the knowledge of the
   outcome variable, which is nevertheless necessary to compute the
   BLUP correction of the stochastic part. This misleading appears
   because of the writing of the reduced form before taking the
   conditional expectation.

\begin{equation}
\tag{prdKP2}\mathbf{E}(y_i\mid X, W, y_{IS})= \rho Wy+ X\widehat{\beta}+ WX\widehat{\theta} 
                                            +  \Omega_{21}\Omega_{11}^{-1}(y_{IS}-\mathbf{E}(y_{IS}\mid X, W))
\end{equation}

     yopla

\begin{equation}
\tag{prdKP2}\mathbf{E}(y_i\mid X, W, y_{IS})= \rho Wy+ X\widehat{\beta}+ WX\widehat{\theta}
                                        + \Omega_{21}\Omega_{11}^{-1}(y_{IS}-\mathbf{E}(y_{IS}\mid X, W))
\end{equation}

   If we put aside the differences in the conditional mean of the
   exogenous part, this predictor =predKP2= is identical to [[ref:prdLSP][prdLSP]].

   Can we still maintain the signal trend distinction? Does it the
   same as direct and indirect effects of covariates?

   We develop a framework of prediction from models with
   interdependent observations.

   We implement the KP1 predictors, also called exogenous by LeSage
   and Pace.

   Custom predictors, see for example \cite{TALG13}

   We have to explain the differences between in-sample, out-of-sample
   and ex-sample in a spatial context. Ex-sample is not necessary
   linked to temporal, it is also interesting to counterfactual
   simulations. The prediction in out-of-sample needs a certain
   spatial embedding between the two spatial samples, not having
   sampled neighbors does not mean no neighbors. But in a spatial
   segregative case, this corresponds to a ex-sample case.

   In the applied literature, one can found KP2 in Dubin (models?), KP4
   in \cite{BCHo07} (models?), LSP in Kato (models?)

** A taxonomy of spatial sample schemes

   In spatial econometrics, the usual distinction between in-sample
   (IS) and out-of-sample (OS) predictions has firstly to be
   refined. It is not simply inside our outside the calibration sample
   but it has also some relations (or not) with it.

* Current function from =spdep=

  Our code is an extension of the function =predict.sarlm()= actually
  the default function from the package =spdep= (Bivand).

#+Name: Lst:DFT
#+begin_src R :results output :file "predict-sarlm.R"
library(spdep) ; predict.sarlm
#+end_src

#+RESULTS: Lst:DFT
[[file:predict-sarlm.R]]

  The current function, accessible through previous link, implement
  different predictor according to the absence of the presence of
  newdata. For the in-sample predictions (=if(newdata=== =NULL)=), the
  predictors are computed as Eq. XX using BLUP. For the out of sample
  predictions (=if(newdata!== =NULL)=), the predictors are computed as
  Eq. XX using biased and inefficient predictors. It produces
  inconsistencies by not implementing the same predictions if we put
  the data that are used to fit the model in the =newdata= argument
  (cf. XX example below). Another shortcoming of the current function
  is the class of objects from SEM and SXM: they are not
  vectors. Lastly, if we put =sacmixed= objects in the current
  function, they are not recognized as such and produce some errors
  about matrix dimension.

  At the center of this distinction is the observability of the
  outcome variable $y$.

  Some other particularities are present in the current function. The
  OS predictor for error models is KP1 but not directly for lag
  models. For that, we have to put =legacy=== =FALSE=. The signal is
  computed by difference for the lag models in out of sample.

* The =sppred= extension
** General Structure

   Here is the general structure of the functions that call
   sub-functions that are defined below.

   This function contents the usual verifications, with 2 more
   arguments: =cond.set= for the conditional set (see XX) and =mean=
   for the specification of the structural mean.

   It is important that the same predictor is implemented when
   newdata are NULL or not, as when spatial matrix set.

   The scan for the lagged WX is by the presence of "lag." at their
   name, it has to be changed.

#+begin_src R :results silent :tangle ./sppred.R
sppred <- function(object, newdata = NULL, listw = NULL, yobs= object$y,
                   condset= "DEF", blup = NULL, loo = FALSE, power = NULL,
                   zero.policy = NULL, legacy = TRUE, order = 250,
                   tol= .Machine$double.eps^(3/5), ...) {
    require(spdep)
    ## USUAL VERIFICATIONS
    if (is.null(zero.policy)) 
        zero.policy <- get("zeroPolicy", envir = spdep:::.spdepOptions)
    stopifnot(is.logical(zero.policy))
    if (is.null(power)) power <- object$method != "eigen"
    stopifnot(is.logical(legacy)) ; stopifnot(is.logical(power))
    ## DETERMINING THE MODEL
    if (object$type== "error"){
        mod <- ifelse(object$etype== "error", "sem", "sxm")
    } else {
        mod <- switch(object$type, "lag"= "sar", "mixed"= "sdm",
                                   "sac"= "sac", "sacmixed"= "smc")
    }
    ## DATA SHAPING
    if (mod %in% c("sem", "sxm")) {lab= object$lambda ; rho= 0         }
    if (mod %in% c("sar", "sdm")) {lab= 0             ; rho= object$rho}
    if (mod %in% c("sac", "smc")) {lab= object$lambda ; rho= object$rho}
    Wlg <- substr(names(object$coefficients), 1, 4)== "lag."
    B <- object$coefficients[ !Wlg] ; Bl <- object$coefficients[ Wlg]
    if (is.null(newdata)){
        X   <- object$X[, !Wlg]
    } else {
        frm <- formula(object$call)
        mt  <- delete.response(terms(frm, data = newdata))
        mf  <- model.frame(mt, newdata)
        X   <- model.matrix(mt, mf)
        if (any(object$aliased)) X <- X[, -which(object$aliased)]
    }
    ## WEIGHT MATRIX, add an error message
    if (is.null(listw)) lsw <- eval(object$call$listw) else lsw <- listw
    ## PREDICTORS
    if (is.null(blup)){
        pt <- switch(condset, "X"= 1, "XW"= 2, "DEF"= 3, "XWy"= 4)
    } else {
        pt <- switch(blup, "LSP"= 5, "KP2"= 6, "KP3"= 7, "KPG"= 8)
    }
    prdX <- as.vector(X %*% B)
    if (pt> 1) prdWX   <- prdWX(prdX, X, Bl, mod, lsw)
    if (pt> 2 && pt!= 4) prdKP1  <- prdKP1(prdWX, rho, lsw, power, order, tol)
    if (pt> 3){
        prdWXy <- prdWX+
            rho* lag.listw(lsw, yobs)+ lab* lag.listw(lsw, yobs- prdWX)}
    if (pt==5) prdLSP <- prdLSP(prdKP1, rho, lab, lsw, yobs, loo)
    if (pt> 5 && !loo) stop("Set loo= TRUE for this blup predictor")
    if (pt==6){
        prdKP2 <- prdKP2(prdKP1, prdWXy,
                         rho, lab, lsw, yobs, power, order, tol)}
    if (pt==7){
        prdKP3 <- prdKP3(prdKP1, prdWXy,
                         rho, lab, lsw, yobs, power, order, tol)}
    if (pt==8) stop("not implemented")
    prd <- switch(pt, "1"= prdX  , "2"= prdWX , "3"= prdKP1, "4"= prdWXy,
                      "5"= prdLSP, "6"= prdKP2, "7"= prdKP3, "8"= prdKPG)
    class(prd) <- "sppred" ; as.vector(prd)
}
#+end_src

   we choose to not use =object$tarX= and =object$tarY= for more
   transparencies. It is clear that we lost from that in terms of
   computation time. It is easy to predict by conditioning only on "X"
   because it is the same form for all the spatial models (see
   equation XX).

** Predictors conditioned on X, W
*** exogenous predictor

#+begin_src R :results silent :tangle ./sppred.R
prdWX <- function(prdX, X= X, Bl= Bl, mod= mod, lsw= lsw){
    if (!mod %in% c("sxm", "sdm", "smc")){
        prdWX <- prdX } else {
            K <- ifelse(colnames(X)[ 1] == "(Intercept)", 2, 1)
            m <- ncol(X) ; WX <- matrix(nrow= length(prdX), ncol= m+ 1- K)
            for (k in K: m){
                WX[, k+ 1- K] <- lag.listw(lsw, X[, k])
            }
            prdWX <- prdX+ (WX %*% Bl)
        } 
    prdWX
}
#+end_src

*** endogenous predictor

#+begin_src R :results silent :tangle ./sppred.R
prdKP1 <- function(prdWX, rho= rho, lsw= lsw,
                   power= power, order= order, tol= tol){
    if (power){
        W <- as(as_dgRMatrix_listw(lsw), "CsparseMatrix")
        prdKP1 <- c(as(powerWeights(W, rho= rho, X= as.matrix(prdWX),
                                    order= order, tol= tol), "matrix"))
    } else {
        prdKP1 <- c(invIrW(lsw, rho) %*% prdWX)
    }
    prdKP1
}
#+end_src

** Predictors conditioned on X, W, y
*** biased predictors

    The predictors equivalent to KP4 and KP5, we do not let the choice
    (because the omitted combination can be recovered from previous
    predictors) and we can eventually add a KP6 for SAC and SMC
    models. The computations are in the general.

*** BLUP LSP

    It can make sens to distinguish one shot to one leave one.


#+begin_src R :results silent :tangle ./sppred.R
prdLSP <- function(prdKP1, rho= rho, lab= lab,
                   lsw= lsw, yobs= yobs, loo= loo){
    ZL <- diag(length(prdKP1))- (lab* listw2mat(lsw))
    ZR <- diag(length(prdKP1))- (rho* listw2mat(lsw))
    Z  <- ZL %*% ZR ; P22 <- t(Z) %*% Z
    if (loo){
        prdLSP <- matrix(NA, ncol= 1, nrow= length(prdKP1))
        for (i in 1: length(prdKP1)){
            prdLSP[ i] <- prdKP1[ i]-
                (P22[i, -i] %*% (yobs[ -i]- prdKP1[ -i])/ P22[i, i])
        }
    } else {
        P11 <- P22
        prdLSP <- prdKP1+ ((solve(P22) %*% P11 %*% (yobs- prdKP1)))
    }
    prdLSP
}
#+end_src

*** BLUP KP2

#+begin_src R :results silent :tangle ./sppred.R
prdKP2 <- function(prdKP1, prdWXy= prdWXy, rho= rho, lab= lab, lsw= lsw,
                   yobs= yobs, power= power, order= order, tol= tol){
    if (power){
        W <- as(as_dgRMatrix_listw(lsw), "CsparseMatrix")
        GL <- as(powerWeights(W, rho= lab, order= order, tol= tol,
                              X= diag(length(prdWXy))), "matrix")
        GR <- as(powerWeights(W, rho= rho, order= order, tol= tol,
                              X= diag(length(prdWXy))), "matrix")
    } else {
        GL <- invIrW(lsw, rho) ; GR <- invIrW(lsw, lab)
    }
    sum.u <- GL %*% t(GL) ; sum.y <- GR %*% sum.u %*% t(GR)
    prdKP2 <- matrix(NA, ncol= 1, nrow= length(prdWXy))
    for (i in 1: length(prdKP2)){
        WM <- listw2mat(lsw)[i, ]
        rg <- (sum.u[i, ] %*% GR %*% WM)/ (WM %*% sum.y %*% WM)
        prdKP2[ i] <- prdWXy[ i]+ (rg %*% WM %*% (yobs- prdKP1))
    }
    prdKP2
}
#+end_src

*** BLUP KP3

#+begin_src R :results silent :tangle ./sppred.R
prdKP3 <- function(prdKP1, prdWXy= prdWXy, rho= rho, lab= lab, lsw= lsw,
                   yobs= yobs, power= power, order= order, tol= tol){
    if (power){
        W <- as(as_dgRMatrix_listw(lsw), "CsparseMatrix")
        GL <- as(powerWeights(W, rho= lab, order= order, tol= tol,
                              X= diag(length(prdWXy))), "matrix")
        GR <- as(powerWeights(W, rho= rho, order= order, tol= tol,
                              X= diag(length(prdWXy))), "matrix")
    } else {
        GL <- invIrW(lsw, lab) ; GR <- invIrW(lsw, rho)
    }
    sum.u <- GL %*% t(GL) ; sum.y <- GR %*% sum.u %*% t(GR)
    prdKP3 <- matrix(NA, ncol= 1, nrow= length(prdWXy))    
    for (i in 1: length(prdKP3)){
        rg <- sum.u[i, ] %*% GR[, -i] %*% solve(sum.y[-i, -i])
        prdKP3[ i] <- prdWXy[ i]+ (rg %*% (yobs[i ]- prdKP1[-i ]))
    }
    prdKP3
}
#+end_src

** Predictors conditioned by hand

   Try with

#+BEGIN_QUOTE
        rg <- sum.u[i, ] %*% GR %*% solve(sum.y)
        prdKP3[ i] <- prdWXy[ i]+ (rg %*% (yobs- prdKP1))
#+END_QUOTE

* How it works
** Choosing a type of predictor

   Our new =R= function for spatial predictions -- called =sppred= for
   the moment -- admits a first additional argument =predictor= that
   specify the computed predictor. Knowing that predictors
   corresponding to larger information sets are more complex,
   flexibility is needed to let the user makes its own trade-off
   between simplicity and prediction efficiency. The following table
   define the available predictors.

#+Caption: The available values for the new =predictor= argument
|-------------+---------------------+-------------------|
| =predictor= | label               | equation (see XX) |
|-------------+---------------------+-------------------|
| "1"         | minimum information | (XX)              |
| "2"         | heuristic BLUP      | (XX)              |
| "3"         | BLUP                | (XX)              |
| "4"         | heuristic data      | (XX)              |
|-------------+---------------------+-------------------|

   The =predictor= 4 is currently the default for IS prediction in
   =predict.sarlm= (it corresponds to the predictor KP4 for lag models
   and KP5 for error models).

** Specifying
** General structure, usual checks, and IS predictions

   Here the code, for the inverse integrating directly the code from
   powerWeigths?

** The predictors 1 for OS predictions   
* The =boston= example
** The spatial samples

   We use here the =boston= dataset, available from the =spdep=
   package (Bivand 2014). It consists in XX. From the whole sample, we
   draw randomly 211 observations from 250 drawn of a uniform
   distribution and dropping duplicated units. This allows to
   distinguish two sample: IS and OS.

#+begin_src R :results output exemple
library(spdep) ; data(boston) ; library(rgdal)
bst.sp <- readOGR(system.file("etc/shapes", package= "spdep"), "boston_tracts")
set.seed(84) ; cal <- unique(round(runif(250)* 500))
bst.sp$SMP <- "OS" ; bst.sp$SMP[cal] <- "IS"
ins.sp <- subset(bst.sp, bst.sp$SMP== "IS")
ots.sp <- subset(bst.sp, bst.sp$SMP== "OS")
cbind("ALL"= dim(bst.sp), "IN SMP"= dim(ins.sp), "OUT SMP"= dim(ots.sp))
#+end_src

#+RESULTS:
: OGR data source with driver: ESRI Shapefile 
: Source: "/home/jsay/R/x86_64-pc-linux-gnu-library/3.0/spdep/etc/shapes", layer: "boston_tracts"
: with 506 features and 21 fields
: Feature type: wkbPolygon with 2 dimensions
:      ALL IN-SMP OUT-SMP
: [1,] 506    211     295
: [2,]  22     22      22

   Spatial units without neighbors are kept in these spatial
   data.frames.

** Neighborhood relationships

   The following code use the function =nbsubset= described in
   Appendix XX and tangled in the file =spsubset.R=. As it is shown by
   the following Figure XX, this function allows to compute the =nb=
   equivalent to the spatial weight matrix $W_I$, $W_O$, $W_{IO}$ and
   $W_{OI}$. The logic behind =nbsubset= is rather different than the
   classical =subset.nb= function because some absent units have to be
   referred by present units. So we do not change the dimension of the
   =nb= matrix, we put some 0 instead but this leads to some problems
   to compute the spatial weights after (see the commentaries in
   Appendix [[SFL]]).

#+Name: Lst:SWB
#+Header: :width 7 :height 7
#+begin_src R :results graphics :file "Figures/SpWeigthBoston.pdf"
source("spsubset.R")
bst.nb <- poly2nb(bst.sp) ; bst.lw <- nb2listw(bst.nb)
WII <- nbsubset(bst.nb, bst.sp$SMP== "IS", target= bst.sp$SMP== "IS")
WIO <- nbsubset(bst.nb, bst.sp$SMP== "IS", target= bst.sp$SMP== "OS")
WOI <- nbsubset(bst.nb, bst.sp$SMP== "OS", target= bst.sp$SMP== "IS")
WOO <- nbsubset(bst.nb, bst.sp$SMP== "OS", target= bst.sp$SMP== "OS")
par(mar= c(0, 0, 0, 0)) ; plot(bst.sp, lwd= 5)
plot(ots.sp, col= "grey70", add= TRUE, border= FALSE)
plot(ins.sp, col= "grey30", add= TRUE, border= FALSE)
plot(bst.nb, coordinates(bst.sp), add= T, col= "black" , pch= 20, cex= .2)
plot(WII, coordinates(bst.sp), add= T, col= "green", pch= 20, cex= .2)
plot(WIO, coordinates(bst.sp), add= T, col= "blue" , pch= 20, cex= .2)
plot(WOO, coordinates(bst.sp), add= T, col= "red"  , pch= 20, cex= .2)
legend("topleft", col= c("blue", "green", "red"), lwd= 1.5,
       c("PRD NBH", "IS NBH", "OS NBH"), bty= "n", cex= 1.25)
legend("right", fill= c("grey30", "grey70"),
       c("IN-SAMPLE", "OUT-OF-SAMPLE"), bty= "n", cex= 1.25)
#+end_src

#+Name: Fig:SWB
#+ATTR_LaTeX: :options scale= .5
#+Caption: Neighboring relationships for the different Boston subsamples 
#+RESULTS: Lst:SWB
[[file:Figures/SpWeigthBoston.pdf]]

   From a first order neighboring point of view, the symmetry of the
   matrix produce a clear distinction between the spatial
   relationships. They are identified through different colors
   according to... The green links are of central interest for
   information transmission between units. Be careful about the island
   that present yellow links (see at the bottom right).

   LES ILOTS "IS" NE PEUVENT PAS ETRE UTILISES POUR ESTIMER LE MODELE
   MAIS PEUVENT ETRE UTILE POUR PREDIRE

   Il ne faut pas que je change les id et donc la dimension il faut
   simplement mettre de zéro? Et ensuite à partir d'autres subsets sur
   cette matrice on devrait obtenir la bonne dimension. Dans cette
   deuxième étape on devrait alors enlever les ilots.

   Une autre possibilité serait de spécifier la matrice avec tout le
   monde, la traduire en W et sélectionner les lignes et les colonnes
   qui nous intéressent.

** Estimating the models

   We estimate on the IS sample the 6 spatial econometric models from
   the =spdep= package, with the classical specification presented in
   Bivand (2002). Before the estimation (and to be OK with the
   computation of spatial weights) we have to drop some units without
   neighbors with the help of the =card= function.

#+begin_src R :results output exemple
## DROP OUT THE UNITS WITHOUT NEIGHBORS
ins.lw <- subset(bst.lw, card(WII)!= 0) 
inz.sp <- subset(bst.sp, card(WII)!= 0)
## DEFINE THE MODEL SPECIFICATION
eqn.bst <- log(CMEDV)~ CRIM    + ZN + INDUS   + CHAS    + I(NOX^2)+
                       I(RM^2) + AGE+ log(DIS)+ log(RAD)+ TAX     +
                       PTRATIO + B  + log(LSTAT)
## ESTIMATE THE MODELS AND REPORT THE AICs
sem <- errorsarlm(eqn.bst, data= inz.sp, ins.lw, method= "Matrix")
sxm <- errorsarlm(eqn.bst, data= inz.sp,
                  ins.lw, etype= "emixed"   , method= "Matrix")
sar <- lagsarlm(  eqn.bst, data= inz.sp, ins.lw, method= "Matrix")
sdm <- lagsarlm(  eqn.bst, data= inz.sp,
                  ins.lw,  type= "mixed"    , method= "Matrix")
sac <- sacsarlm(  eqn.bst, data= inz.sp, ins.lw, method= "Matrix")
smc <- sacsarlm(  eqn.bst, data= inz.sp,
                  ins.lw,  type= "sacmixed" , method= "Matrix")
spmod <- list(sem= sem, sxm= sxm, sar= sar, sdm= sdm, sac= sac, smc= smc)
data.frame(lapply(spmod, AIC))
#+end_src

#+RESULTS:
:         sem       sxm       sar       sdm       sac       smc
: 1 -132.4525 -164.1509 -139.7872 -163.6526 -142.8058 -163.1881

   The SXM is the more parsimonious in terms of AC 

** Predicting for in-sample

   We next compute the RMSE for each model $\times$ predictor
   combination.

#+begin_src R :results value exemple :rownames yes :colnames yes
source("sppred.R") ; library(plyr)
rmse <- function(prd, mod) sqrt(mean(I(prd- mod$y)^2))
RMSE <- data.frame("X"=   ldply(spmod, function(x)
                           rmse(sppred(x, condset= "X"), x))$V1,
              "XW"=  ldply(spmod, function(x)
                           rmse(sppred(x, condset= "XW"), x))$V1,
              "KP1"= ldply(spmod, function(x) rmse(sppred(x), x))$V1,
              "XWy"= ldply(spmod, function(x)
                           rmse(sppred(x, condset= "XWy"), x))$V1,
              "LSP"= ldply(spmod, function(x)
                           rmse(sppred(x, blup= "LSP", loo= TRUE), x))$V1,
              "KP2"= ldply(spmod, function(x)
                           rmse(sppred(x, blup= "KP2", loo= TRUE), x))$V1,
              "KP3"= ldply(spmod, function(x)
                           rmse(sppred(x, blup= "KP3", loo= TRUE), x))$V1)
row.names(RMSE) <- names(spmod) ; round(RMSE, 4)
#+end_src

#+ATTR_LaTeX: :placement [htb]\small
#+Caption: The RMSE from IS sample for the 6 spatial models and 7 predictors
#+RESULTS:
|     |      X |     XW |    KP1 |    XWy |    LSP |    KP2 |    KP3 |
|-----+--------+--------+--------+--------+--------+--------+--------|
| sem | 0.2124 | 0.2124 | 0.2124 | 0.1449 |  0.137 |  0.185 | 0.2015 |
| sxm | 0.9832 | 0.1661 | 0.1661 | 0.1292 | 0.1221 | 0.1464 | 0.1539 |
| sar | 1.3541 | 1.3541 | 0.1846 | 0.1504 | 0.1409 | 0.1423 | 0.1119 |
| sdm | 1.5871 | 1.5536 | 0.1627 | 0.1306 | 0.1231 |  0.128 | 0.1127 |
| sac |  0.941 |  0.941 | 0.1899 | 0.3152 | 0.1373 |  0.327 |  0.302 |
| smc | 1.9819 |  0.692 |   0.17 | 0.4797 | 0.1208 | 0.4632 | 0.4714 |

   The results are intriguing, the fact that KP3 does not always
   perform best is fuzzy.

** Recovering impact measures
** Recovering trend and signal terms
* Testing							   :noexport:
** Sample

#+Name: Lst:PSS
#+Header: :width 11 :height 4
#+begin_src R :results graphics :file "Figures/PrsSpSmp.pdf"
load("Data/exsmp.Rda") ; library(spdep)
plot(exsmp$Dat.all)
plot(exsmp$Dat.cal, col= "blue", pch= 20, add= TRUE)
#+end_src

#+Name: Fig:PSS
#+ATTR_LaTeX: :options scale= .5
#+Caption: Calibration and exhaustive datasets
#+RESULTS: Lst:PSS
[[file:Figures/PrsSpSmp.pdf]]
 
** Estimating the spatial models

#+begin_src R :results output exemple
SEM <- errorsarlm(ARlog03~ PXLB03+ RTFO03+ BdAlti, data= exsmp$Dat.cal,
                  exsmp$Wgt.cal, method= "eigen")
SXM <- errorsarlm(ARlog03~ PXLB03+ RTFO03+ BdAlti, data= exsmp$Dat.cal,
                  exsmp$Wgt.cal, method= "eigen", etype= "emixed")
SAR <- lagsarlm(  ARlog03~ PXLB03+ RTFO03+ BdAlti, data= exsmp$Dat.cal,
                  exsmp$Wgt.cal, method= "eigen")
SDM <- lagsarlm(  ARlog03~ PXLB03+ RTFO03+ BdAlti, data= exsmp$Dat.cal,
                  exsmp$Wgt.cal, method= "eigen", type= "mixed")
SAC <- sacsarlm(  ARlog03~ PXLB03+ RTFO03+ BdAlti, data= exsmp$Dat.cal,
                  exsmp$Wgt.cal, method= "eigen")
SMC <- sacsarlm(  ARlog03~ PXLB03+ RTFO03+ BdAlti, data= exsmp$Dat.cal,
                  exsmp$Wgt.cal, method= "eigen", type= "sacmixed")
SPMOD <- list(SEM= SEM, SXM= SXM, SAR= SAR, SDM= SDM, SAC= SAC, SMC= SMC)
rbind(lapply(spmod, AIC))
#+end_src

#+RESULTS:
:      sem       sxm       sar       sdm       sac       smc      
: [1,] -132.4525 -164.1509 -139.7872 -163.6526 -142.8058 -163.1881

** Constructing spatial weight matrix

   En friche

#+begin_src R :results output exemple
str(exsmp$Dat.cal)
attr(exsmp$Wgt.cal, "region.id")
attr(exsmp$Wgt.all, "region.id")
attr(exsmp$Dat.cal@coords, "dimnames")
attr(exsmp$Dat.cal@coords, "dimnames")
mrgOS <- coordinates(exsmp$Dat.all)
mrgIS <- coordinates(exsmp$Dat.cal)
yop <- merge(mrgOS)
#+end_src

** Testing the predictors
*** Conditioned on X

#+begin_src R :results output exemple
source("sppred.R")
SEMprdX <- sppred(SEM, condset= "X")
sqrt(mean(I(SEMprdX- SEM$y)^2))
SXMprdX <- sppred(SXM, condset= "X")
sqrt(mean(I(SXMprdX- SXM$y)^2))
SARprdX <- sppred(SAR, condset= "X")
sqrt(mean(I(SARprdX- SAR$y)^2))
SDMprdX <- sppred(SDM, condset= "X")
sqrt(mean(I(SDMprdX- SDM$y)^2))
SACprdX <- sppred(SAC, condset= "X")
sqrt(mean(I(SACprdX- SAC$y)^2))
SMCprdX <- sppred(SMC, condset= "X")
sqrt(mean(I(SMCprdX- SMC$y)^2))
SMCprdX <- sppred(SMC, newdata= exsmp$Dat.cal, condset= "X", power= T)
sqrt(mean(I(SMCprdX- SMC$y)^2))

## A ESSAYER AVEC L'AUTRE BASE SPATIALE
#+end_src

*** Conditioned on X, W

#+begin_src R :results output exemple
source("sppred.R")
SEMprdWX <- sppred(SEM, condset= "XW")
sqrt(mean(I(SEMprdWX- SEM$y)^2))
SXMprdWX <- sppred(SXM, condset= "XW")
sqrt(mean(I(SXMprdWX- SXM$y)^2))
SARprdWX <- sppred(SAR, condset= "XW")
sqrt(mean(I(SARprdWX- SAR$y)^2))
SDMprdWX <- sppred(SDM, condset= "XW")
sqrt(mean(I(SDMprdWX- SAR$y)^2))
SACprdWX <- sppred(SAC, condset= "XW")
sqrt(mean(I(SACprdWX- SAR$y)^2))
SMCprdWX <- sppred(SMC, condset= "XW")
sqrt(mean(I(SMCprdWX- SAR$y)^2))
SXMprdXW <- sppred(SXM, newdata= exsmp$Dat.cal,
                   condset= "XW", listw= exsmp$Wgt.cal)
sqrt(mean(I(SXMprdWX- SXM$y)^2))
#+end_src

*** KP1 predictors

#+begin_src R :results output exemple
source("sppred.R")
SEMprdKP1 <- sppred(SEM)
sqrt(mean(I(SEMprdKP1- SEM$y)^2))
SXMprdKP1 <- sppred(SXM)
sqrt(mean(I(SXMprdKP1- SXM$y)^2))
SARprdKP1 <- sppred(SAR)
sqrt(mean(I(SARprdKP1- SAR$y)^2))
SDMprdKP1 <- sppred(SDM)
sqrt(mean(I(SDMprdKP1- SAR$y)^2))
SACprdKP1 <- sppred(SAC)
sqrt(mean(I(SACprdKP1- SAR$y)^2))
SMCprdKP1 <- sppred(SMC)
sqrt(mean(I(SMCprdKP1- SAR$y)^2))

yop <- sppred(SXM, condset= "XW")
WWW <- as(as_dgRMatrix_listw(exsmp$Wgt.cal), "CsparseMatrix")
pp <- c(as(powerWeights(WWW, rho= 0, X= as.matrix(yop)), "matrix"))

SXMprdKP1 <- sppred(SXM, newdata= exsmp$Dat.cal)
sqrt(mean(I(SXMprdKP1- SXM$y)^2))
SXMprdKP1 <- sppred(SXM, power= TRUE)
SXMprdKP1 <- sppred(SXM, newdata= exsmp$Dat.cal, listw= exsmp$Wgt.cal)
SXMprdKP1 <- sppred(SXM, newdata= exsmp$Dat.cal, listw= exsmp$Wgt.cal,
                    power= TRUE)
sqrt(mean(I(SXMprdKP1- SXM$y)^2))

#+end_src

*** Biased predictor

#+begin_src R :results output exemple
source("sppred.R")
SEMprdXWy <- sppred(SEM, condset= "XWy")
sqrt(mean(I(SEMprdX- SEM$y)^2))
sqrt(mean(I(SEMprdWX- SEM$y)^2))
sqrt(mean(I(SEMprdKP1- SEM$y)^2))
sqrt(mean(I(SEMprdXWy- SEM$y)^2))

SXMprdXWy <- sppred(SXM, condset= "XWy")
sqrt(mean(I(SXMprdX- SXM$y)^2))
sqrt(mean(I(SXMprdXW- SXM$y)^2))
sqrt(mean(I(SXMprdKP1- SXM$y)^2))
sqrt(mean(I(SXMprdXWy- SXM$y)^2))

SARprdXWy <- sppred(SAR, condset= "XWy")
sqrt(mean(I(SARprdX- SAR$y)^2))
sqrt(mean(I(SARprdWX- SAR$y)^2))
sqrt(mean(I(SARprdKP1- SAR$y)^2))
sqrt(mean(I(SARprdXWy- SAR$y)^2))

SDMprdXWy <- sppred(SDM, condset= "XWy")
sqrt(mean(I(SDMprdX- SDM$y)^2))
sqrt(mean(I(SDMprdWX- SDM$y)^2))
sqrt(mean(I(SDMprdKP1- SDM$y)^2))
sqrt(mean(I(SDMprdXWy- SDM$y)^2))

SACprdXWy <- sppred(SAC, condset= "XWy")
sqrt(mean(I(SACprdX- SAC$y)^2))
sqrt(mean(I(SACprdWX- SAC$y)^2))
sqrt(mean(I(SACprdKP1- SAC$y)^2))
sqrt(mean(I(SACprdXWy- SAC$y)^2))

SMCprdXWy <- sppred(SMC, condset= "XWy")
sqrt(mean(I(SMCprdX- SMC$y)^2))
sqrt(mean(I(SMCprdWX- SMC$y)^2))
sqrt(mean(I(SMCprdKP1- SMC$y)^2))
sqrt(mean(I(SMCprdXWy- SMC$y)^2))

SDMprdX <- sppred(SDM, condset= "XWy", power= TRUE)
SDMprdX <- sppred(SDM, condset= "XWy", newdata= exsmp$Dat.cal)
sqrt(mean(I(SDMprdXWy- SDM$y)^2))

## ON PEUT COMMENCER A METTRE DU yobs choisit
#+end_src

*** LSP predictor

#+begin_src R :results output exemple
source("sppred.R")
SEMprdKP1 <- sppred(SEM, blup= "LSP")
sqrt(mean(I(SEMprdKP1- SEM$y)^2))

SEMprdKP1 <- sppred(SEM, blup= "LSP", loo= TRUE)
sqrt(mean(I(SEMprdKP1- SEM$y)^2))


SXMprdXWy <- sppred(SXM, blup= "LSP", loo= TRUE)
sqrt(mean(I(SXMprdXWy- SXM$y)^2))

SARprdXWy <- sppred(SAR, condset= "XWy")
sqrt(mean(I(SARprdXWy- SAR$y)^2))
SDMprdXWy <- sppred(SDM, condset= "XWy")
sqrt(mean(I(SDMprdXWy- SDM$y)^2))
SACprdXWy <- sppred(SAC, condset= "XWy")
sqrt(mean(I(SACprdXWy- SAC$y)^2))
SMCprdXWy <- sppred(SMC, condset= "XWy")
sqrt(mean(I(SMCprdXWy- SMC$y)^2))

SDMprdX <- sppred(SDM, condset= "XWy", power= TRUE)
SDMprdX <- sppred(SDM, condset= "XWy", newdata= exsmp$Dat.cal)
sqrt(mean(I(SDMprdXWy- SDM$y)^2))

## ON PEUT COMMENCER A METTRE DU yobs choisit
#+end_src

*** KP2 predictor

#+begin_src R :results output exemple
source("sppred.R")
SEMprdKP2 <- sppred(SEM, blup= "KP2", loo= TRUE)
sqrt(mean(I(SEMprdKP2- SEM$y)^2))
SXMprdKP2 <- sppred(SXM, blup= "KP2", loo= TRUE)
sqrt(mean(I(SXMprdKP2- SXM$y)^2))
SARprdKP2 <- sppred(SAR, blup= "KP2", loo= TRUE)
sqrt(mean(I(SARprdKP2- SAR$y)^2))
SDMprdKP2 <- sppred(SDM, blup= "KP2", loo= TRUE)
sqrt(mean(I(SDMprdKP2- SDM$y)^2))
SACprdKP2 <- sppred(SAC, blup= "KP2", loo= TRUE)
sqrt(mean(I(SACprdKP2- SAC$y)^2))
SMCprdKP2 <- sppred(SMC, blup= "KP2", loo= TRUE)
sqrt(mean(I(SMCprdKP2- SMC$y)^2))

SMCprdKP2 <- sppred(SMC, condset= "X", blup= "KP2", loo= TRUE)
sqrt(mean(I(SMCprdKP2- SMC$y)^2))

## ON PEUT COMMENCER A METTRE DU yobs choisit
#+end_src

*** KP3 predictor

#+begin_src R :results output exemple
source("sppred.R")
SEMprdKP3 <- sppred(SEM, blup= "KP3", loo= TRUE)
sqrt(mean(I(SEMprdKP3- SEM$y)^2))
yop <- sppred(SEM, blup= "LSP", loo= TRUE)
sqrt(mean(I(yop- SEM$y)^2))

system.time(SXMprdKP3 <- sppred(SXM, blup= "KP3", loo= TRUE, power= FALSE))
sqrt(mean(I(SXMprdKP3- SXM$y)^2))
system.time(yop <- sppred(SXM, blup= "LSP", loo= TRUE, power= TRUE))
sqrt(mean(I(yop- SXM$y)^2))


system.time(SARprdKP3 <- sppred(SAR, blup= "KP3", loo= TRUE))
sqrt(mean(I(SARprdKP3- SAR$y)^2))
system.time(yop <- sppred(SAR, blup= "LSP", loo= TRUE, power= TRUE))
sqrt(mean(I(yop- SAR$y)^2))

a <- SARprdKP3- sppred(SAR, condset= "XWy")
b <- yop- sppred(SAR)
plot(a, b)


SDMprdKP3 <- sppred(SDM, blup= "KP3", loo= TRUE)
sqrt(mean(I(SDMprdKP3- SDM$y)^2))
SACprdKP3 <- sppred(SAC, blup= "KP3", loo= TRUE)
sqrt(mean(I(SACprdKP3- SAC$y)^2))
SMCprdKP3 <- sppred(SMC, blup= "KP3", loo= TRUE, power= TRUE)
sqrt(mean(I(SMCprdKP3- SMC$y)^2))
yop <- sppred(SMC, blup= "LSP", loo= TRUE)
sqrt(mean(I(yop- SMC$y)^2))




SMCprdKP3 <- sppred(SMC, condset= "X",
                    newdata= exsmp$Dat.cal[ 1: 10,],
                    blup= "KP3", loo= TRUE)
sqrt(mean(I(SMCprdKP3- SMC$y)^2))

yop <- sppred(SMC, blup= "LSP", loo= TRUE)
sqrt(mean(I(SMCprdKP3- SMC$y)^2))

plot(yop, SMCprdKP3)

## ON PEUT COMMENCER A METTRE DU yobs choisit
#+end_src

** Proof of Harville's simplification

   To be implemented we need to be in the case of ex-sample
   prediction. Hopefully, our data.frame allows that.

#+begin_src R :results output exemple
load("Data/exsmp.Rda") ; library(spdep)
WCAL <- listw2mat(exsmp$Wgt.cal) ; WALL <- listw2mat(exsmp$Wgt.all)
CAL <- row.names(exsmp$Dat.all) %in% row.names(exsmp$Dat.cal)
## BIZARRE: yop <- WALL[CAL, CAL] != WCAL
## ON N'UTILISE QUE WALL DE TOUTE FACON
## METHODE PAR LA COVARIANCE
Z <- (diag(dim(WALL)[ 1])- (.33* WALL))
OMG <- solve(Z) %*% solve(t(Z)) # isSymmetric(OMG) TRUE
OMG11 <- OMG[ CAL,  CAL] ; OMG22 <- OMG[!CAL, !CAL]
OMG12 <- OMG[ CAL, !CAL] ; OMG21 <- OMG[!CAL,  CAL]
BLUP1 <- OMG21 %*% solve(OMG11)
BLUP1[1: 10, 1: 10]
## METHODE PAR LA MATRICE DE PRECISION
PHI <- t(Z) %*% Z # isSymmetric(PHI) TRUE
PHI11 <- PHI[ CAL,  CAL] ; PHI22 <- PHI[!CAL, !CAL]
PHI12 <- PHI[ CAL, !CAL] ; PHI21 <- PHI[!CAL,  CAL]
BLUP2 <- - solve(PHI22) %*% PHI21
BLUP2[1: 10, 1: 10]
#+end_src

   It is OK. It stay the question about using the computing capacity
   of =spdep= in this framework.

* Summary
** Changes relative to =predict.sarlm=

   - Implement predictions for SARAR and Mixed SARAR models from
     respectively =sac= and =sacmixed= classes.
   - Compute BLUP and almost BLUP spatial predictors
   - About the in-sample / out of sample structure (=newdata=)
   - About the distinction between trend and signal
   - The simplification of the in-sample predictions

** About the intercept

   We change the scan of the intercept, in particular in presence of
   $WX$ in the regression. If $W$ is row standardized, we have to drop
   the intercept to avoid collinearity. The initial function add the
   constant at the end of the computations, we only drop the intercept
   in the presence of $WX$.

* Bibliography							  :noheading:

#+BIBLIOGRAPHY: BibPrd erae2 limit:t option:-nokeys option:-nofooter
#+LaTeX: \appendix

* Appendix
** Subset function for =nb= objects

   The following function =nbsubset= is a first step to decompose the
   spatial weight matrix, in order to compute ex-sample predictions.
   Based on the function =subset.nb= from the package =spdep=, the
   =subset= argument is replaced by two new arguments: =focal= and
   =target=.  In raw matrix terms, =focal= corresponds to rows and
   =target= corresponds to columns.  As =subset=, these arguments have
   to be logical vector of the same size that the =nb= object.  Note
   that we can recover the same =nb= relations from the =subset.nb= by
   putting the same logical vector both for =focal= and =target=. See
   XX for an example of use.

#+begin_src R :tangle ./spsubset.R
nbsubset <- function (x, focal= rep(TRUE, length(x)),
                      target= rep(TRUE, length(x)), ...){
    if (!inherits(x, "nb")) 
        stop("not a neighbours list")
    if (!is.logical(focal)) 
        stop("subset not a logical vector")
    if (!is.logical(target)) 
        stop("target not a logical vector")
    n <- length(x)
    if (n != length(focal)) 
        stop("neighours list and subset focal different lengths")
    if (n != length(target)) 
        stop("neighours list and subset target different lengths")
    tgt.ids <- seq(1, n)[ target]
    x <- sym.attr.nb(x)
    xattrs <- names(attributes(x))
    z <- ifelse(focal, x, 0L)
    for (i in seq(along= 1: n)[ focal]){
        zi <- z[[ i]]
        zn <- zi[zi %in% tgt.ids]
        if (length(zn)== 0) z[[ i]] <- 0L
        else z[[ i]] <- sort(unique(zn))
    }
    attr(z, "region.id") <-attr(x, "region.id")
    for (i in 1:length(xattrs)) {
        if (xattrs[i] != "region.id") 
            attr(z, xattrs[i]) <- attr(x, xattrs[i])
    }
    z <- sym.attr.nb(z)
    z
}
#+end_src

** << SFL>> Subset function for =listw= objects

   Below the =lwsubset= function, that is the symmetric of previous
   =nbsubset= function for the =listw= objects. The code is not
   finished because of the function =nb2listw= that cannot admits
   units without neighbors and the presence of rectangular =nb=
   relationships from =nbsubset=.

#+begin_src R :tangle ./spsubset.R
lwsubset <- function (x, focal= rep(TRUE, length(x)),
                      target= rep(TRUE, length(x)), zero.policy = NULL, ...){
    if (!inherits(x, "listw")) 
        stop("not a weights list")
    if (is.null(zero.policy)) 
        zero.policy <- get("zeroPolicy", envir = .spdepOptions)
    stopifnot(is.logical(zero.policy))
    if (!is.logical(focal)) 
        stop("focal not a logical vector")
    if (!is.logical(target)) 
        stop("target not a logical vector")
    nb <- x$neighbours
    vlist <- x$weights
    if (attr(vlist, "mode") != "binary") 
        stop("Not yet able to subset general weights lists")
    style <- x$style
    n <- length(nb)
    if (n != length(focal)) 
        stop("neighbours list and focal vector different lengths")
    if (n != length(target)) 
        stop("neighbours list and target vector different lengths")
    subnb <- nbsubset(nb, focal, target)
    sublistw <- nb2listw(neighbours= subnb, glist= NULL,
                         style= style, zero.policy= zero.policy)
    sublistw
}
#+end_src

* Footnotes

[fn:1] This terminology comes from LeSage and Pace (2009) and Bivand
  (2014). This model is also called spatial autoregressive model with
  autoregressive disturbances, SARAR(1,1), by Kelejian and Prucha
  (1998). The notations of spatial lag coefficients also depends on
  authors, we adopt here those of the *R* package =spdep= (Bivand,
  2014).

[fn:2] In particular, it could be of interest to predict outcome
  values for units with known $y$ for diagnostic, goodness-of-fit
  purposes or computations of marginal effects of covariates.

[fn:3] Does the geo-statistical literature is concerned with marginal
  effects of covariates? Somme papers?

